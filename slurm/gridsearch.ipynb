{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b70ff64-38e7-418d-a871-c1b543e4e841",
   "metadata": {},
   "source": [
    "## Use slurm to perform Hyperparameter Optimization with GridSearch\n",
    "\n",
    "This script performs a systematic search for the optimal hyperparameters for a Support Vector Machine (SVM) classifier. \n",
    "\n",
    "### Why do we use a cluster?\n",
    "\n",
    "The cluster allows us to distribute computationally intensive tasks across multiple computers to save time and increase efficiency. In hyperparameter optimization, where potentially thousands of combinations need to be tested, using a cluster can significantly speed up the process.\n",
    "\n",
    "### Why do we store results in a database?\n",
    "\n",
    "Since jobs on a cluster can be interrupted for various reasons (such as system failures, maintenance, or time limits), we store the results in a PostgreSQL database. This allows us to resume the optimization process, without having to start over. Each parameter combination and its result are stored so that combinations can be skipped if the script is restarted.\n",
    "\n",
    "### Key components of the script\n",
    "\n",
    "- **Establishing a database connection**: We use `psycopg2` to connect to our PostgreSQL database. \n",
    "- **Creating a data table**: If not already present, a table is created in the database to store the results.\n",
    "- **Defining the parameter space**: We define a space of possible values for the hyperparameters `C`, `gamma`, and `kernel` to be assigned to the SVM.\n",
    "- **Evaluation**: For each combination of parameters in the `ParameterGrid`, we perform cross-validation and save the results in the database.\n",
    "- **Retrieving results**: After all computations are complete, we query the database for the best parameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2e415ca-68ab-423a-bab6-b5dc9f7a0c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gridsearch.py\n"
     ]
    }
   ],
   "source": [
    "%%file gridsearch.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score, ParameterGrid\n",
    "from sklearn.svm import SVC\n",
    "import psycopg2\n",
    "import json\n",
    "\n",
    "# Datenbankverbindung herstellen\n",
    "conn = psycopg2.connect(\n",
    "    dbname='postgres',\n",
    "    user='postgres.ymxgukzcysicyvmmouxx',\n",
    "    password='PW',\n",
    "    host='aws-0-eu-central-1.pooler.supabase.com',\n",
    "    port='5432'\n",
    ")\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Datenbanktabelle erstellen, falls noch nicht vorhanden\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS gridsearch_results (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    params JSON,\n",
    "    mean_score FLOAT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Datensatz laden\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Parameter-Raum definieren\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Prüfen, welche Parameter bereits evaluiert wurden\n",
    "cur.execute(\"SELECT params FROM gridsearch_results\")\n",
    "evaluated_params = {json.dumps(row[0]) for row in cur.fetchall()}\n",
    "\n",
    "# ParameterGrid erstellen und durch Iteration Parameter evaluieren\n",
    "param_list = list(ParameterGrid(param_grid))\n",
    "\n",
    "\n",
    "for params in param_list:\n",
    "    params_json = json.dumps(params)\n",
    "    if params_json not in evaluated_params:\n",
    "        model = SVC(**params)\n",
    "        score = np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1))\n",
    "        # Ergebnis in die Datenbank schreiben\n",
    "        cur.execute(\n",
    "            \"INSERT INTO gridsearch_results (params, mean_score) VALUES (%s, %s)\",\n",
    "            (params_json, score)\n",
    "        )\n",
    "        conn.commit()\n",
    "        print(f\"Evaluated: {params}, Score: {score}\")\n",
    "\n",
    "# Beste Ergebnisse abrufen\n",
    "cur.execute(\"SELECT params, mean_score FROM gridsearch_results ORDER BY mean_score DESC LIMIT 1\")\n",
    "best_result = cur.fetchone()\n",
    "print(f\"Beste Parameter: {best_result[0]}, Beste Score: {best_result[1]}\")\n",
    "\n",
    "# Datenbankverbindung schließen\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f0a8c-a11f-499b-84e4-99ae0caaefb4",
   "metadata": {},
   "source": [
    "## Slurm Script\n",
    "\n",
    "The slurm script below configures job scheduling parameters and the computational environment needed to execute the grid search.\n",
    " \n",
    "- **Shebang (`#!/bin/bash`)**: Specifies that the script should be run in the Bash shell.\n",
    "- **`#SBATCH` Directives**: These lines configure the resources and settings for the Slurm job scheduler:\n",
    "  - `--job-name=gridsearch`: Sets the job's name to 'gridsearch', which helps in identifying the job within the job queue.\n",
    "  - `--output=./logs/gridsearch_%j.out`: Directs the standard output (stdout) of the job to a file in the `logs` directory. The `%j` is replaced by the job ID, allowing for unique log files for different job executions.\n",
    "  - `--error=./logs/gridsearch_%j.err`: Similar to the output directive, this directs standard error (stderr) to a file, helping in debugging if the job encounters issues.\n",
    "  - `--time=01:00:00`: Limits the job's maximum running time to one hour. If the job exceeds this time, it will be terminated by Slurm.\n",
    "  - `--cpus-per-task=4`: Allocates 4 CPU cores to the job, which is particularly useful for tasks that can exploit parallel processing.\n",
    "  - `--mem=4G`: Assigns 4 gigabytes of RAM to the job, ensuring sufficient memory is available for processing.\n",
    "  - `--partition=base`: Specifies the partition where the job should run. The 'base' partition is a general-purpose queue configured on the cluster.\n",
    "  \n",
    "- **Environment Setup**:\n",
    "  - `module load python/3.8`: Loads the Python 3.8 module, setting up the software environment required to run the Python script.\n",
    "  - `pip install --user scikit-learn pandas psycopg2-binary`: Installs the necessary Python packages in the user's home directory.\n",
    "\n",
    "- **Script Execution**:\n",
    "  - `python gridsearch.py`: This line is the core task, using all the previously configured settings and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55dfff82-3244-4688-ad68-0159227a38e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run_gridsearch.slurm\n"
     ]
    }
   ],
   "source": [
    "%%file run_gridsearch.slurm\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=gridsearch\n",
    "#SBATCH --output=./logs/gridsearch_%j.out\n",
    "#SBATCH --error=./logs/gridsearch_%j.err\n",
    "#SBATCH --time=01:00:00\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --mem=4G\n",
    "#SBATCH --partition=base\n",
    "\n",
    "module load python/3.8\n",
    "pip install --user scikit-learn pandas psycopg2-binary\n",
    "\n",
    "python gridsearch.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd12e3b-69cf-4356-8197-4de44a75b113",
   "metadata": {},
   "source": [
    "## Prepare execution\n",
    "The `sinfo` command provides detailed information about the state of the cluster's nodes and partitions. \n",
    "\n",
    "You can use `sinfo` to quickly check the status of the cluster's partitions and nodes. This is especially useful when planning to submit large jobs, as you can identify which partitions are least busy and what limits you are working within. \n",
    "\n",
    "### Understanding `sinfo` Output\n",
    "\n",
    "When you run `sinfo`, it provides several key pieces of information:\n",
    "\n",
    "- **PARTITION**: The name of the partition. Partitions are subsets of the cluster, often configured with specific types of nodes or for particular groups or job types.\n",
    "- **AVAIL**: Indicates whether the partition is available for job submission.\n",
    "- **TIMELIMIT**: Shows the maximum duration that jobs are allowed to run in the partition.\n",
    "- **NODES**: Lists the number of nodes in each state within the partition.\n",
    "- **STATE**: Displays the current state of nodes. Common states include `idle` (available for new jobs), `alloc` (allocated to a job), `down` (not operational), `drain` (being removed from active duty, usually for maintenance), etc.\n",
    "- **NODELIST**: Identifies the specific nodes within each state.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca58611a-3a6e-4869-9447-fd1ed89ea8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
      "base*        up 14-00:00:0      3  down* lab-aicl-n[1,11,17]\n",
      "base*        up 14-00:00:0      1   comp lab-aicl-n16\n",
      "base*        up 14-00:00:0      6  alloc lab-aicl-n[2-3,5-6,12,19]\n",
      "base*        up 14-00:00:0      9   idle lab-aicl-n[4,7-10,13-15,18]\n"
     ]
    }
   ],
   "source": [
    "!sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040e920-7589-439e-bc89-2208d5a54b28",
   "metadata": {},
   "source": [
    "## Submitting a script to the  Cluster\n",
    "\n",
    "When you run `sbatch run_gridsearch.slurm`, the Slurm scheduler receives and processes the script. It schedules and executes the job according to the resources specified in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acad7a02-5529-4c50-a76e-70f83da9c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 202646\n"
     ]
    }
   ],
   "source": [
    "!sbatch run_gridsearch.slurm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
